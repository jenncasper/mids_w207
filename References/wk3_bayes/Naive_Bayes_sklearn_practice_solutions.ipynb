{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris target names: ['setosa' 'versicolor' 'virginica']\n",
      "Iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print 'Iris target names:', iris.target_names\n",
    "print 'Iris feature names:', iris.feature_names\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sklearn has three types of Naive Bayes: gaussian, beroulli, and multinomial.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "What is the difference between them? These are the assumed ditributional form of each component of P(X|Y); the distribution of each individual features.\n",
    "\n",
    "Try using each of these on the iris data, you will have to prepare the data for multinomial and bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian accuracy: 0.96\n",
      "bernoulli accuracy: 0.66\n",
      "multinomial accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/numpy-1.11.2-py2.7-macosx-10.11-intel.egg/numpy/core/fromnumeric.py:2699: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "gau = GaussianNB()\n",
    "gau.fit(train_data, train_labels)\n",
    "\n",
    "bern = BernoulliNB(binarize=0.9)\n",
    "bern.fit(train_data, train_labels)\n",
    "\n",
    "mult = MultinomialNB()\n",
    "# floor converts data into a discrete-like set\n",
    "mult.fit(np.floor(train_data), train_labels)\n",
    "\n",
    "print 'gaussian accuracy: %3.2f' %gau.score(test_data, test_labels)\n",
    "print 'bernoulli accuracy: %3.2f' %bern.score(test_data, test_labels)\n",
    "print 'multinomial accuracy: %3.2f' %mult.score(np.floor(test_data), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What choices did you make with manipulating the features above? Try tuning these choices, can you improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarize: 1.0 bernoulli accuracy: 0.66\n",
      "binarize: 2.0 bernoulli accuracy: 0.88\n",
      "binarize: 3.0 bernoulli accuracy: 0.80\n",
      "binarize: 4.0 bernoulli accuracy: 0.66\n",
      "binarize: 5.0 bernoulli accuracy: 0.74\n",
      "binarize: 6.0 bernoulli accuracy: 0.62\n",
      "binarize: 7.0 bernoulli accuracy: 0.46\n",
      "binarize: 8.0 bernoulli accuracy: 0.28\n",
      "binarize: 9.0 bernoulli accuracy: 0.28\n"
     ]
    }
   ],
   "source": [
    "# lets play around with binarize in bernoulli\n",
    "\n",
    "for bin in [x / 1.0 for x in range(1, 10)]:\n",
    "    bern = BernoulliNB(binarize=bin)\n",
    "    bern.fit(train_data, train_labels)\n",
    "    print 'binarize:', bin, 'bernoulli accuracy: %3.2f' %bern.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate what effect alpha has on the bernoulli and multinomial classifiers. What happens when alpha is very high? Is there an optimal value?\n",
    "\n",
    "Does increasing alpha add bias or variance to our estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0 bernoulli accuracy: 0.38;\n",
      "alpha: 0.001 bernoulli accuracy: 0.34;\n",
      "alpha: 0.01 bernoulli accuracy: 0.34;\n",
      "alpha: 0.1 bernoulli accuracy: 0.34;\n",
      "alpha: 1 bernoulli accuracy: 0.34;\n",
      "alpha: 5 bernoulli accuracy: 0.34;\n",
      "alpha: 10 bernoulli accuracy: 0.34;\n",
      "alpha: 50 bernoulli accuracy: 0.28;\n",
      "alpha: 100 bernoulli accuracy: 0.28;\n",
      "alpha: 1000 bernoulli accuracy: 0.28;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/naive_bayes.py:820: RuntimeWarning: divide by zero encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Library/Python/2.7/site-packages/sklearn/naive_bayes.py:823: RuntimeWarning: invalid value encountered in add\n",
      "  jll += self.class_log_prior_ + neg_prob.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# alpha is a biasing effect, e.g.\n",
    "\n",
    "for aa in [0, 0.001, 0.01, 0.1, 1, 5, 10, 50, 100, 1000]:\n",
    "    bern = BernoulliNB(binarize=0.1, alpha=aa)\n",
    "    bern.fit(train_data, train_labels)\n",
    "    \n",
    "    print 'alpha:', aa, 'bernoulli accuracy: %3.2f;' %bern.score(test_data, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
