{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Added libraries\n",
    "from time import time\n",
    "from fractions import Fraction\n",
    "from pprint import pprint\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "\n",
    "fetch_20newsgroups takes time to execute - a few minutes on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:1 comp.graphics\n",
      "----------Start Msg\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "----------End Msg\n",
      "Label:3 talk.religion.misc\n",
      "----------Start Msg\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "----------End Msg\n",
      "Label:2 sci.space\n",
      "----------Start Msg\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "----------End Msg\n",
      "Label:0 alt.atheism\n",
      "----------Start Msg\n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "----------End Msg\n",
      "Label:2 sci.space\n",
      "----------Start Msg\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "----------End Msg\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "# References: N/A\n",
    "# Time to complete: ~1hr\n",
    "\n",
    "    # loop through the first 5 training records and output\n",
    "    for i in range(num_examples):\n",
    "        print(\"Label:%s %s\\n----------Start Msg\\n%s\\n----------End Msg\" % (train_labels[i],\n",
    "                                                                           newsgroups_train.target_names[int(train_labels[i])],\n",
    "                                                                           train_data[i]))\n",
    "\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "\n",
    "The messages are \"messy\" with special characters, numbers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.003s.\n",
      "Done in 1.170s.\n",
      "\n",
      "('Shape of the tf sparse matrix: ', (2034, 26879))\n",
      "The 2,034 document collection has a 26,879 token vocabulary.\n",
      "\n",
      "('Shape of the tf2 sparse matrix: ', (2034, 4))\n",
      "The 2,034 document collection has a 4 token vocabulary.\n",
      "\n",
      "Average number of non-zero features per example in tf: 96.71\n",
      "Fraction of 54,671,886 matrix elements that are non-zero in tf: 0.36%\n",
      "\n",
      "First feature in the alpha sorted list for tf: 00\n",
      "Last feature in the alpha sorted list fot tf: zyxel\n",
      "\n",
      "Average number of non-zero features per example in tf2: 0.27\n",
      "Fraction of 8,136 matrix elements that are non-zero in tf2: 6.71%\n",
      "\n",
      "time\tanalyzer\tngram max\ttotal vocab size\n",
      "3.731s\tchar\t\t1\t\t79\n",
      "7.171s\tchar\t\t2\t\t3,370\n",
      "10.927s\tchar\t\t3\t\t35,557\n",
      "0.850s\tword\t\t1\t\t26,879\n",
      "4.898s\tword\t\t2\t\t221,770\n",
      "9.043s\tword\t\t3\t\t537,462\n",
      "\n",
      "Done in 0.993s.\n",
      "The 2,034 document collection has a 3,064 token vocabulary with min_df=10.\n",
      "\n",
      "Done in 0.091s.\n",
      "Out of 16,246 total feature names from dev_data, 4,027 or 0.248 are missing from the train_data vocabulary.\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "# References:\n",
    "#   http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    # turn the raw training text into feature vectors without any additional options in the CountVectorizer\n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(train_data)\n",
    "    print(\"Done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    # vectorize the raw training text using a limited vocabulary\n",
    "    vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "    tf_vectorizer2 = CountVectorizer(vocabulary=vocab)\n",
    "    t0 = time()\n",
    "    tf2 = tf_vectorizer2.fit_transform(train_data)\n",
    "    print(\"Done in %0.3fs.\\n\" % (time() - t0))\n",
    "    \n",
    "    # what is the size of the vocabulary?\n",
    "    print(\"Shape of the tf sparse matrix: \", tf.shape)\n",
    "    print(\"The %s document collection has a %s token vocabulary.\\n\" % ('{:,}'.format(tf.shape[0]), \n",
    "                                                                     '{:,}'.format(tf.shape[1])))\n",
    "    print(\"Shape of the tf2 sparse matrix: \", tf2.shape)\n",
    "    print(\"The %s document collection has a %s token vocabulary.\\n\" % ('{:,}'.format(tf2.shape[0]), \n",
    "                                                                     '{:,}'.format(tf2.shape[1])))\n",
    "    \n",
    "    # what is the average number of non-zero features per example?\n",
    "    totalnnz = 0.0\n",
    "    totalnnz2 = 0.0\n",
    "    for i in range(len(train_data)):\n",
    "        totalnnz = totalnnz + float(tf[i].getnnz())\n",
    "        totalnnz2 = totalnnz2 + float(tf2[i].getnnz())\n",
    "    #print(totalnnz)\n",
    "    #print(totalnnz2)\n",
    "    print \"Average number of non-zero features per example in tf: %.2f\" % (totalnnz/float(tf.shape[0]))\n",
    "    \n",
    "    # what fraction of the entries in the matrix are non-zero?\n",
    "    totalelts = int(tf.shape[0])*int(tf.shape[1])\n",
    "    print(\"Fraction of %s matrix elements that are non-zero in tf: %s\\n\" % ('{:,}'.format(totalelts), \n",
    "                                                                    '{:.2%}'.format(totalnnz/float(totalelts))))\n",
    "    \n",
    "    # what are the 0th and last feature strings in alpha order?\n",
    "    sorted_tfvec = sorted(tf_vectorizer.get_feature_names())\n",
    "    print(\"First feature in the alpha sorted list for tf: %s\" % (sorted_tfvec[0]))\n",
    "    print(\"Last feature in the alpha sorted list fot tf: %s\\n\" % (sorted_tfvec[-1]))\n",
    "\n",
    "    # what is the average number of non-zero features per example with a constrained vocabulary?\n",
    "    print(\"Average number of non-zero features per example in tf2: %.2f\" % (totalnnz2/float(tf2.shape[0])))\n",
    "    totalelts2 = int(tf2.shape[0])*int(tf2.shape[1])\n",
    "    print(\"Fraction of %s matrix elements that are non-zero in tf2: %s\\n\" % ('{:,}'.format(totalelts2), \n",
    "                                                                    '{:.2%}'.format(totalnnz2/float(totalelts2))))\n",
    "    \n",
    "    # What size vocabulary do bigram and trigram character features yield?\n",
    "    params = (('char', 1), ('char', 2), ('char', 3), ('word', 1), ('word', 2), ('word', 3))\n",
    "    print(\"time\\tanalyzer\\tngram max\\ttotal vocab size\")\n",
    "    for item in params:\n",
    "        tf_vectorizer3 = CountVectorizer(analyzer=item[0], ngram_range=(1,item[1]))\n",
    "        t0 = time()\n",
    "        tf3 = tf_vectorizer3.fit_transform(train_data)\n",
    "        print(\"%0.3fs\\t%s\\t\\t%s\\t\\t%s\" % ((time()-t0), item[0], item[1], '{:,}'.format(tf3.shape[1])))\n",
    "    \n",
    "    # What size vocabulary does min_df of <10 yield?\n",
    "    tf_vectorizer4 = CountVectorizer(min_df=10)\n",
    "    t0 = time()\n",
    "    tf4 = tf_vectorizer4.fit_transform(train_data)\n",
    "    print(\"\\nDone in %0.3fs.\" % (time() - t0))\n",
    "    print(\"The %s document collection has a %s token vocabulary with min_df=10.\\n\" % ('{:,}'.format(tf4.shape[0]), \n",
    "                                                                     '{:,}'.format(tf4.shape[1]))) \n",
    "    \n",
    "    # What fraction of the words in the dev data are missing from the vocabulary?\n",
    "    tf_vectorizer5 = CountVectorizer()\n",
    "    tf5 = tf_vectorizer5.fit_transform(dev_data)\n",
    "    #print(tf5.shape)\n",
    "    #print(len(tf_vectorizer5.get_feature_names()))\n",
    "    t0 = time()\n",
    "    missing = set(tf_vectorizer5.get_feature_names())-set(tf_vectorizer.get_feature_names())\n",
    "    print(\"Done in %0.3fs.\" % (time() - t0))\n",
    "    num_miss = len(missing)\n",
    "    frac = float(num_miss)/float(tf5.shape[1])\n",
    "    print(\"Out of %s total feature names from dev_data, %s or %0.3f are missing from the train_data vocabulary.\" % ('{:,}'.format(tf5.shape[1]), \n",
    "                                                                                                          '{:,}'.format(num_miss),\n",
    "                                                                                                          frac))\n",
    "\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWERS:\n",
    "\n",
    "a.1 What is the size of the vocabulary? The 2034 document collection has a 26,879 token vocabulary.\n",
    "a.2 What is the average number of non-zero features per example? 96.71\n",
    "a.3 What fraction of the entries in the matrix are non-zero? 0.36%\n",
    "\n",
    "b.1 What are the 0th and last feature strings (in alphabetical order)? 00, zyxel\n",
    "\n",
    "c.1 What is the average number of non-zero features per example with a constrained vocabulary? 6.71%\n",
    "\n",
    "d.1 What size vocabulary do bigram and trigram character features yield? See the pseudo-chart outputted above for reference. The original bigram word feature count is listed at 26,879. Noice how the char feature count starts at 79 for unigram, increases to 3,370 for unigram+bigram, and increases again to 35,557 for unigram+bigram+trigram. \n",
    "\n",
    "e.1 What size vocabulary does min_df of less than 10 yield? 3,064 unigram word feature count; this is significantly less than the original 26,879.\n",
    "\n",
    "f.1 What fraction of the words in the dev data are missing from the vocabulary? 4,027 out of the 16,246 feature names in the dev_data, or 0.248, are missing from the train_data vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "In calculating the fraction of non-zero entries in the matrix, I assume the total matrix size is row*col. \n",
    "\n",
    "The features, without any constraints or preprocessing, are incredibly messy. This is seen when the sorted feature list was printed out - all sorts of non-alpha characters.\n",
    "\n",
    "It is unclear what the analyzer parameter actually does - is it defining what feature types are selected? The documentation says when an analyzer or pre-defined vocabulary are not used, the number of features resulting from processing is the equivalent of vocabulary size. The default feature value appears to be 'word'. Changing it to 'char' greatly reduces the size of the vocabulary - presumably due to just the ngram character strings counting as features.\n",
    "\n",
    "The ngram_range parameter by default is (1,1) thus only bigrams are allowed. Changing it to (1,2) or (1,3) increases the vocabulary size as bigrams and trigrams are now included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Performing grid search for KNN\n",
      "('Pipeline:', ['vect', 'knn'])\n",
      "Parameters:\n",
      "{'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)}\n",
      "\n",
      "\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   32.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 33.881s\n",
      "\n",
      "Grid scores: [mean: 0.40905, std: 0.01882, params: {'knn__n_neighbors': 1}, mean: 0.40315, std: 0.02001, params: {'knn__n_neighbors': 2}, mean: 0.39577, std: 0.00974, params: {'knn__n_neighbors': 3}, mean: 0.39971, std: 0.02267, params: {'knn__n_neighbors': 4}, mean: 0.40905, std: 0.02529, params: {'knn__n_neighbors': 5}, mean: 0.40364, std: 0.03466, params: {'knn__n_neighbors': 6}, mean: 0.40806, std: 0.02397, params: {'knn__n_neighbors': 7}, mean: 0.39479, std: 0.02727, params: {'knn__n_neighbors': 8}, mean: 0.41052, std: 0.01514, params: {'knn__n_neighbors': 9}, mean: 0.41347, std: 0.01946, params: {'knn__n_neighbors': 10}, mean: 0.40757, std: 0.00691, params: {'knn__n_neighbors': 11}, mean: 0.41495, std: 0.02038, params: {'knn__n_neighbors': 12}, mean: 0.41396, std: 0.02103, params: {'knn__n_neighbors': 13}, mean: 0.41544, std: 0.01685, params: {'knn__n_neighbors': 14}, mean: 0.41396, std: 0.01495, params: {'knn__n_neighbors': 15}]\n",
      "\n",
      "<type 'list'>\n",
      "Best score: 0.415\n",
      "Best parameters set:\n",
      "\tknn__n_neighbors: 14\n",
      "\n",
      "Classification report for CountVectorizer and KNN GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...owski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.33      0.44      0.38       165\n",
      "     comp.graphics       0.46      0.46      0.46       185\n",
      "         sci.space       0.55      0.48      0.52       199\n",
      "talk.religion.misc       0.31      0.25      0.28       127\n",
      "\n",
      "       avg / total       0.43      0.42      0.42       676\n",
      "\n",
      "\n",
      "KNN f1_score: 0.422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P3_a():\n",
    "# References:\n",
    "#  http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "#  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "    # use CountVectorizer, find the optimal value for k in KNN, and report the f1 score for KNN using metrics.f1_score\n",
    "    \n",
    "    # set up the pipeline\n",
    "    knn_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "    ])\n",
    "    \n",
    "    # define parameter values to run through\n",
    "    knn_params = {\n",
    "        'knn__n_neighbors': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),\n",
    "    }\n",
    "    \n",
    "    # set up and execute GridSearch to fit on training data\n",
    "    grid_search = GridSearchCV(knn_pipe, knn_params, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"-----------Performing grid search for KNN\")\n",
    "    print(\"Pipeline:\", [name for name, _ in knn_pipe.steps])\n",
    "    print(\"Parameters:\")\n",
    "    pprint(knn_params)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    t0 = time()\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    print(\"Done in %0.3fs\\n\" % (time() - t0))\n",
    "    \n",
    "    print(\"Grid scores: %s\\n\" % grid_search.grid_scores_)\n",
    "    print(type(grid_search.grid_scores_))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(knn_params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    # run GridSearch against the dev data\n",
    "    predicted = grid_search.predict(dev_data)\n",
    "    expected = dev_labels\n",
    "    report = classification_report(expected, predicted, target_names=newsgroups_test.target_names)\n",
    "    print(\"\\nClassification report for CountVectorizer and KNN %s:\\n%s\\n\" % (grid_search, report))\n",
    "    \n",
    "    # precision - number of positive predictions divided by the total number of positive class values predicted\n",
    "    # recall - number of positive predictions divided by the number of positive class values in the test data\n",
    "    # f1 - conveys the balance between the precision and the recall\n",
    "    #print(metrics.f1_score(expected, predicted, average=None))\n",
    "    #print(metrics.f1_score(expected, predicted, average='macro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='micro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='weighted'))\n",
    "    print(\"KNN f1_score: %0.3f\\n\" % (grid_search.score(dev_data, dev_labels)))\n",
    "\n",
    "P3_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Performing grid search for NB\n",
      "('Pipeline:', ['vect', 'nb'])\n",
      "Parameters:\n",
      "{'nb__alpha': (0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0)}\n",
      "\n",
      "\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 17.004s\n",
      "\n",
      "Grid scores: [mean: 0.82694, std: 0.00969, params: {'nb__alpha': 0.0001}, mean: 0.82596, std: 0.00718, params: {'nb__alpha': 0.001}, mean: 0.82940, std: 0.00822, params: {'nb__alpha': 0.01}, mean: 0.82350, std: 0.01417, params: {'nb__alpha': 0.1}, mean: 0.81072, std: 0.01420, params: {'nb__alpha': 0.5}, mean: 0.79744, std: 0.01651, params: {'nb__alpha': 1.0}, mean: 0.77483, std: 0.01916, params: {'nb__alpha': 2.0}, mean: 0.68977, std: 0.01035, params: {'nb__alpha': 10.0}]\n",
      "\n",
      "Best score: 0.829\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.01\n",
      "\n",
      "Classification report for CountVectorizer and MultinomialNB GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'nb__alpha': (0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.67      0.72      0.69       165\n",
      "     comp.graphics       0.92      0.90      0.91       185\n",
      "         sci.space       0.81      0.89      0.85       199\n",
      "talk.religion.misc       0.65      0.50      0.57       127\n",
      "\n",
      "       avg / total       0.78      0.78      0.78       676\n",
      "\n",
      "\n",
      "NB f1_score: 0.780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P3_b():\n",
    "# References:\n",
    "#  http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "    # fit a Multinomial Naive Bayes model and find optimal value for alpha\n",
    "    \n",
    "    # set up the pipeline\n",
    "    nb_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('nb', MultinomialNB()),\n",
    "    ])\n",
    "    \n",
    "    # define parameter values to run through\n",
    "    nb_params = {\n",
    "        'nb__alpha': (0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0),\n",
    "    }\n",
    "\n",
    "    # set up and execute GridSearch to fit on training data\n",
    "    grid_search = GridSearchCV(nb_pipe, nb_params, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"-----------Performing grid search for NB\")\n",
    "    print(\"Pipeline:\", [name for name, _ in nb_pipe.steps])\n",
    "    print(\"Parameters:\")\n",
    "    pprint(nb_params)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    t0 = time()\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    print(\"Done in %0.3fs\\n\" % (time() - t0))\n",
    "    \n",
    "    print(\"Grid scores: %s\\n\" % grid_search.grid_scores_)\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(nb_params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # run GridSearch against the dev data\n",
    "    predicted = grid_search.predict(dev_data)\n",
    "    expected = dev_labels\n",
    "    report = classification_report(expected, predicted, target_names=newsgroups_test.target_names)\n",
    "    print(\"\\nClassification report for CountVectorizer and MultinomialNB %s:\\n%s\\n\" % (grid_search, report))\n",
    "    \n",
    "    #print(metrics.f1_score(expected, predicted, average=None))\n",
    "    #print(metrics.f1_score(expected, predicted, average='macro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='micro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='weighted'))\n",
    "    print(\"NB f1_score: %0.3f\\n\" % (grid_search.score(dev_data, dev_labels)))\n",
    "    \n",
    "P3_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Performing grid search for LogReg\n",
      "('Pipeline:', ['vect', 'logreg'])\n",
      "Parameters:\n",
      "{'logreg__C': (0.01, 0.07, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0)}\n",
      "\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:   35.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 37.396s\n",
      "\n",
      "Grid scores: [mean: 0.73402, std: 0.00750, params: {'logreg__C': 0.01}, mean: 0.76696, std: 0.00271, params: {'logreg__C': 0.07}, mean: 0.76647, std: 0.00299, params: {'logreg__C': 0.1}, mean: 0.77089, std: 0.00230, params: {'logreg__C': 0.3}, mean: 0.77139, std: 0.00337, params: {'logreg__C': 0.5}, mean: 0.77040, std: 0.00312, params: {'logreg__C': 0.6}, mean: 0.76893, std: 0.00361, params: {'logreg__C': 0.7}, mean: 0.76844, std: 0.00306, params: {'logreg__C': 0.8}, mean: 0.76254, std: 0.00237, params: {'logreg__C': 1.0}]\n",
      "\n",
      "Best score: 0.771\n",
      "Best parameters set:\n",
      "\tlogreg__C: 0.5\n",
      "\n",
      "Classification report for CountVectorizer and LogisticRegression GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'logreg__C': (0.01, 0.07, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.56      0.59       165\n",
      "     comp.graphics       0.80      0.88      0.84       185\n",
      "         sci.space       0.75      0.82      0.79       199\n",
      "talk.religion.misc       0.61      0.52      0.56       127\n",
      "\n",
      "       avg / total       0.71      0.71      0.71       676\n",
      "\n",
      "\n",
      "LogisticRegression f1_score: 0.714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P3_c1():\n",
    "# References:\n",
    "#  http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    \n",
    "    # fit a logistic regression model and find optimal value for regularization strength C using l2 regularization\n",
    "\n",
    "    # set up the pipeline\n",
    "    logreg_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('logreg', LogisticRegression(penalty='l2')),\n",
    "    ])\n",
    "    \n",
    "    # define parameter values to run through\n",
    "    \n",
    "    logreg_params = {\n",
    "        'logreg__C': (0.01, 0.07, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0),\n",
    "    }\n",
    "    \n",
    "    # set up and execute GridSearch to fit on training data\n",
    "    grid_search = GridSearchCV(logreg_pipe, logreg_params, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"-----------Performing grid search for LogReg\")\n",
    "    print(\"Pipeline:\", [name for name, _ in logreg_pipe.steps])\n",
    "    print(\"Parameters:\")\n",
    "    pprint(logreg_params)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    t0 = time()\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    print(\"Done in %0.3fs\\n\" % (time() - t0))\n",
    "    \n",
    "    print(\"Grid scores: %s\\n\" % grid_search.grid_scores_)\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(logreg_params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    # run GridSearch against the dev data\n",
    "    predicted = grid_search.predict(dev_data)\n",
    "    expected = dev_labels\n",
    "    report = classification_report(expected, predicted, target_names=newsgroups_test.target_names)\n",
    "    print(\"\\nClassification report for CountVectorizer and LogisticRegression %s:\\n%s\\n\" % (grid_search, report))\n",
    "       \n",
    "    #print(metrics.f1_score(expected, predicted, average=None))\n",
    "    #print(metrics.f1_score(expected, predicted, average='macro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='micro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='weighted'))\n",
    "    print(\"LogisticRegression f1_score: %0.3f\\n\" % (grid_search.score(dev_data, dev_labels)))\n",
    "\n",
    "P3_c1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing done in 0.910s.\n",
      "\n",
      "-----------Sum of squared weight values for each class for each C value\n",
      "         C        time      alt.atheism         comp.graphics        sci.space  talk.religion.misc\n",
      "     0.001       0.47s        0.1650935             0.2009527        0.1806709   0.1872428\n",
      "     0.112       1.03s       30.0573659            27.0515793       30.3152365  25.5561847\n",
      "     0.223       1.40s       54.4674462            46.4352908       53.8381759  46.8109355\n",
      "     0.334       1.75s       75.4247916            62.5644922       73.6871872  65.1349515\n",
      "     0.445       1.26s       94.0608259            76.6716515       91.0908877  81.4845667\n",
      "     0.556       2.00s      110.9381323            89.3045610      106.7129977  96.3195129\n",
      "     0.667       1.36s      126.4522253           100.8916370      121.0758544  109.9632892\n",
      "     0.778       1.43s      140.8396916           111.5641734      134.1533836  122.6654895\n",
      "     0.889       1.52s      154.3375972           121.5173124      146.5051665  134.5280233\n",
      "       1.0       1.46s      166.9848420           130.8913711      157.9747928  145.7513297\n"
     ]
    }
   ],
   "source": [
    "def P3_c2():\n",
    "# References:\n",
    "    \n",
    "    # vectorize the the training and dev data\n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    print(\"Vectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    \n",
    "    print(\"-----------Sum of squared weight values for each class for each C value\")\n",
    "    print('%10s  %10s  %15s  %20s  %15s  %10s' % ('C', 'time', newsgroups_train.target_names[0], \n",
    "                                                  newsgroups_train.target_names[1], \n",
    "                                                  newsgroups_train.target_names[2], \n",
    "                                                  newsgroups_train.target_names[3]))\n",
    "    \n",
    "    # train a model using logreg     \n",
    "    for c in np.linspace(0.001, 1.0, 10): \n",
    "        logreg = LogisticRegression(\n",
    "            C=c\n",
    "        )   \n",
    "        t0 = time()\n",
    "        logreg = logreg.fit(tf_train, train_labels)\n",
    "        #print(\"\\nLogreg fitting done in %0.3fs.\" % (time() - t0))\n",
    "        \n",
    "        # logistic regression estimates a weight vector for each class, which you can access with the coef_ attribute \n",
    "        # output the sum of the squared weight values for each class for each setting of the C parameter \n",
    "        print('%10s  %10s  %15s  %20s  %15s  %10s' % (c, '{:.2f}s'.format(time() - t0),\n",
    "                                                      '{:.7f}'.format(np.sum(logreg.coef_[0]**2)),\n",
    "                                                      '{:.7f}'.format(np.sum(logreg.coef_[1]**2)),\n",
    "                                                      '{:.7f}'.format(np.sum(logreg.coef_[2]**2)),\n",
    "                                                      '{:.7f}'.format(np.sum(logreg.coef_[3]**2))))\n",
    "        #print logreg.coef_.shape\n",
    "        #print(np.count_nonzero(logreg.coef_))\n",
    "    \n",
    "P3_c2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWERS:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "KNN has a very low score relative to NB and Logreg. KNN is not probablistic in that it will review closest neighbors every time a predication is made. KNN will likely be sensitive to the context of neighboring documents. Preprocessing may help improve KNN.\n",
    "\n",
    "KNN f1_score: 0.422\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "Naive Bayes weighs each of the features correlation with the class independently. Logistic regression will weight all the features together as they compare to the class. Logistic regression will be more \"skeptical\" for weighting features as it takes into account the other features that influence the classification.\n",
    "\n",
    "NB f1_score: 0.780\n",
    "LogisticRegression f1_score: 0.714\n",
    "\n",
    "c. Briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "The sum of squared weights for each of the classes increases as the value of C increases. As the C increases, the affect the penalty has on the log function decreases. The C value refines the affect the penalty has on the log function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "I would really like to go over the three questions above in class. The minute I believe I understand - I lose the concept such that I am not confident I can explain \"why\" to others.\n",
    "\n",
    "GridSearchCV and Pipeline made a lot more sense this time around. I spent more time experimenting with the values and verboseness of the functions to get an understanding of what it is doing. I haven't yet determined what the \"fitting 3 folds for the candidates\" means. I assume it isn't splitting the training data up into groups, but I am not sure.\n",
    "\n",
    "The version of GridSearchCV is from the grid_search library rather than the model_selection library. I'd like to eventually make use of the model_selection version for the cv_results method and output.\n",
    "\n",
    "The code used to execute KNN and NB could be cleaned up and minimized. I left the code repetitive so I could see all of the steps performed and manipulate as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing done in 1.182s.\n",
      "\n",
      "Logreg fitting done in 32.462s.\n",
      "\n",
      "     feature       index      alt.atheism         comp.graphics        sci.space  talk.religion.misc\n",
      "    atheists       3,870            0.154                -0.037           -0.051      -0.075\n",
      "       bobby       4,784            0.154                -0.039           -0.049      -0.055\n",
      "       islam      13,668            0.166                -0.043           -0.059      -0.047\n",
      "    religion      20,430            0.196                -0.100           -0.135       0.026\n",
      "     atheism       3,866            0.211                -0.077           -0.060      -0.109\n",
      "    computer       6,555           -0.045                 0.215           -0.106      -0.064\n",
      "         any       3,392           -0.036                 0.223           -0.133      -0.081\n",
      "       image      12,769           -0.089                 0.227           -0.121      -0.060\n",
      "        file      10,376           -0.068                 0.245           -0.147      -0.075\n",
      "    graphics      11,552           -0.156                 0.416           -0.238      -0.112\n",
      "      launch      14,540           -0.068                -0.080            0.181      -0.049\n",
      "        moon      16,358           -0.078                -0.093            0.190      -0.042\n",
      "       orbit      17,597           -0.072                -0.111            0.212      -0.053\n",
      "        nasa      16,697           -0.102                -0.112            0.236      -0.082\n",
      "       space      22,567           -0.230                -0.288            0.566      -0.189\n",
      "       order      17,609           -0.090                -0.011           -0.029       0.116\n",
      "   objective      17,282           -0.059                -0.065           -0.079       0.134\n",
      "  christians       5,904           -0.079                -0.047           -0.058       0.138\n",
      "         who      26,270            0.019                -0.177           -0.041       0.160\n",
      "   christian       5,901           -0.097                -0.061           -0.083       0.178\n",
      "\n",
      "Vectorizing done in 4.688s.\n",
      "\n",
      "Logreg with bigrams fitting done in 136.797s.\n",
      "\n",
      "                feature       index      alt.atheism         comp.graphics        sci.space  talk.religion.misc\n",
      "                 30 fps       3,870           -0.002                 0.005           -0.002      -0.001\n",
      "                42 does       4,784           -0.001                 0.004           -0.002      -0.001\n",
      "           an initworld      13,668           -0.000                 0.002           -0.001      -0.000\n",
      "             are stable      20,430           -0.001                -0.002            0.005      -0.001\n",
      "            30 exhibits       3,866           -0.001                 0.006           -0.003      -0.001\n",
      "               90 bucks       6,555           -0.001                -0.001            0.003      -0.001\n",
      "                 25 edt       3,392           -0.001                -0.000            0.002      -0.001\n",
      "              am fairly      12,769            0.004                -0.005            0.004      -0.003\n",
      "           against tyre      10,376           -0.002                -0.000           -0.001       0.002\n",
      "               all then      11,552           -0.002                -0.003            0.005      -0.001\n",
      "           and chemical      14,540           -0.002                -0.003            0.006      -0.001\n",
      "           and relaxing      16,358            0.007                -0.003           -0.002      -0.001\n",
      "      another insulting      17,597           -0.000                -0.000           -0.001       0.001\n",
      "              and stick      16,697            0.005                 0.001           -0.004      -0.002\n",
      "               aslv 150      22,567           -0.001                -0.001            0.002      -0.001\n",
      "   another mathematical      17,609            0.006                -0.002           -0.002      -0.002\n",
      "         angles compare      17,282           -0.001                -0.001            0.003      -0.001\n",
      "             70 million       5,904           -0.000                -0.000            0.000      -0.000\n",
      "              be direct      26,270            0.000                -0.000           -0.000      -0.000\n",
      "               70 knots       5,901           -0.000                -0.000            0.001      -0.000\n"
     ]
    }
   ],
   "source": [
    "def P4():\n",
    "# References:\n",
    "#   N/A\n",
    "\n",
    "    # turn the raw training text into feature vectors without any additional options in the CountVectorizer\n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    tf_names = tf_vectorizer.get_feature_names()\n",
    "    tf_dev = tf_vectorizer.transform(dev_data)\n",
    "    print(\"Vectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(tf_dev.shape)\n",
    "    #print(dev_labels.shape)\n",
    "    #print len(tf_names)\n",
    "    \n",
    "    # train a logistic regression model\n",
    "    logregCV = LogisticRegressionCV(\n",
    "        #Cs=(0.01, 0.07, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0)\n",
    "        Cs=np.linspace(0.001, 0.01, 10)\n",
    "    )\n",
    "    t0 = time()\n",
    "    logregCV = logregCV.fit(tf_train, train_labels)\n",
    "    #print(logregCV.get_params())\n",
    "    print(\"Logreg fitting done in %0.3fs.\\n\" % (time() - t0))\n",
    "    \n",
    "    #print(np.count_nonzero(logregCV.coef_))\n",
    "    #print(logregCV.coef_)\n",
    "    #print(logregCV.coef_.shape)    \n",
    "    #print(\"LogisticRegression f1_score: %0.3f\\n\" % (logregCV.score(tf_dev, dev_labels)))\n",
    "    \n",
    "    # find 5 features with the largest weights for each label - 20 features in total\n",
    "    # create a table with 20 rows and 4 columns that show the weight for each of these features for each of the labels\n",
    "    top_inds = []\n",
    "    top_features = []\n",
    "    for row in logregCV.coef_:\n",
    "        inds = np.argsort(row)\n",
    "        top_inds.extend(inds[-5:])\n",
    "        top_features.extend([tf_names[i] for i in inds[-5:]])\n",
    "    #print len(top_inds)\n",
    "    #print len(top_features)\n",
    "    \n",
    "    print('%12s  %10s  %15s  %20s  %15s  %10s' % ('feature', 'index', newsgroups_train.target_names[0], \n",
    "                                                  newsgroups_train.target_names[1], \n",
    "                                                  newsgroups_train.target_names[2], \n",
    "                                                  newsgroups_train.target_names[3]))\n",
    "    for i in range(20): print('%12s  %10s  %15s  %20s  %15s  %10s' % (top_features[i], '{:,}'.format(top_inds[i]), \n",
    "                                                                     '{:.3f}'.format(logregCV.coef_[0][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV.coef_[1][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV.coef_[2][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV.coef_[3][top_inds[i]])))\n",
    "\n",
    "    \n",
    "    # create the table again with bigram features\n",
    "    tf_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    tf_names = tf_vectorizer.get_feature_names()\n",
    "    tf_dev = tf_vectorizer.transform(dev_data)\n",
    "    print(\"\\nVectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(tf_dev.shape)\n",
    "    #print(dev_labels.shape)\n",
    "    #print len(tf_names)\n",
    "    \n",
    "    # train a logistic regression model using bigram words\n",
    "    logregCV2 = LogisticRegressionCV(\n",
    "        #Cs=(0.01, 0.07, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 1.0)\n",
    "        Cs=np.linspace(0.001, 0.01, 10)\n",
    "    )\n",
    "    t0 = time()\n",
    "    logregCV2 = logregCV2.fit(tf_train, train_labels)\n",
    "    print(\"Logreg with bigrams fitting done in %0.3fs.\\n\" % (time() - t0))\n",
    "    \n",
    "    top_inds = []\n",
    "    top_features = []\n",
    "    for row in logregCV.coef_:\n",
    "        inds = np.argsort(row)\n",
    "        top_inds.extend(inds[-5:])\n",
    "        top_features.extend([tf_names[i] for i in inds[-5:]])\n",
    "    \n",
    "    print('%23s  %10s  %15s  %20s  %15s  %10s' % ('feature', 'index', newsgroups_train.target_names[0], \n",
    "                                                  newsgroups_train.target_names[1], \n",
    "                                                  newsgroups_train.target_names[2], \n",
    "                                                  newsgroups_train.target_names[3]))\n",
    "    for i in range(20): print('%23s  %10s  %15s  %20s  %15s  %10s' % (top_features[i], '{:,}'.format(top_inds[i]), \n",
    "                                                                     '{:.3f}'.format(logregCV2.coef_[0][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV2.coef_[1][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV2.coef_[2][top_inds[i]]),\n",
    "                                                                     '{:.3f}'.format(logregCV2.coef_[3][top_inds[i]])))\n",
    "\n",
    "P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "a. Any surprising features in the bigram table?\n",
    "\n",
    "The weights in the unigram table for the top 20 features made seemed to map to the topics. The weights coincided with the classes as well. The bigram table weights were much smaller, likely due to the greater number of bigram features, and not as clearly matched to the topic. I had thought the bigram features may have had more obvious top features. However, the vocabulary includes all words that may muddy the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "The categories were labeled by order. I created a sub_categories list to map the labels to strings.\n",
    "\n",
    "The code is verbose so I could explicitly see all of the steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing done in 1.271s.\n",
      "\n",
      "Vectorizing done in 2.319s.\n",
      "\n",
      "The 2,034 document collection has a 26,879 token vocabulary.\n",
      "\n",
      "The 2,034 document collection has a 22,933 token cleaned up vocabulary.\n",
      "\n",
      "Vocabulary reduced by 14.68%.\n",
      "\n",
      "\n",
      "Logreg fitting done in 116.634s.\n",
      "Score for dev data: 0.707101\n",
      "\n",
      "Logreg fitting with preprocessing done in 38.597s.\n",
      "Score for dev data with preprocessing: 0.723373\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    # strip punctuation, underscores, and short words, and lowercase the resulting words\n",
    "    return re.sub(ur'\\b\\w{1,4}\\b','',re.sub(ur\"[^\\w\\d\\s]+|\\d+|_+\",'',s)).lower()\n",
    "\n",
    "def P5():\n",
    "# References:\n",
    "\n",
    "    # turn the raw training text into feature vectors without any additional options in the CountVectorizer\n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    tf_dev = tf_vectorizer.transform(dev_data)\n",
    "    print(\"Vectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(tf_dev.shape)\n",
    "    #print(dev_labels.shape)\n",
    "    \n",
    "    # turn the raw training text into feature vectors with preprocessing and options in the CountVectorizer\n",
    "    tf_vectorizer2 = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True,\n",
    "        preprocessor=better_preprocessor,\n",
    "        stop_words='english',\n",
    "        max_df=0.8,\n",
    "        #min_df=0.01,\n",
    "    )\n",
    "    t0 = time()\n",
    "    tf2_train = tf_vectorizer2.fit_transform(train_data)\n",
    "    tf2_dev = tf_vectorizer2.transform(dev_data)\n",
    "    print(\"Vectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf2_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(tf2_dev.shape)\n",
    "    #print(dev_labels.shape)\n",
    "    \n",
    "    # what is the size of the vocabulary?\n",
    "    print(\"The %s document collection has a %s token vocabulary.\\n\" % ('{:,}'.format(tf_train.shape[0]), \n",
    "                                                                     '{:,}'.format(tf_train.shape[1])))\n",
    "    print(\"The %s document collection has a %s token cleaned up vocabulary.\\n\" % ('{:,}'.format(tf2_train.shape[0]), \n",
    "                                                                     '{:,}'.format(tf2_train.shape[1])))\n",
    "    \n",
    "    print(\"Vocabulary reduced by %s.\\n\" % ('{:.2f}%'.format(((float(tf_train.shape[1])-float(tf2_train.shape[1]))/float(tf_train.shape[1]))*100.0)))\n",
    "\n",
    "    # calculate the scores using dev_data for comparison\n",
    "    logreg = LogisticRegressionCV(\n",
    "        Cs=np.linspace(0.01, 1.0, 20)\n",
    "    )\n",
    "    t0 = time()\n",
    "    logreg = logreg.fit(tf_train, train_labels)\n",
    "    print(\"\\nLogreg fitting done in %0.3fs.\" % (time() - t0))\n",
    "    print(\"Score for dev data: %f\" % (logreg.score(tf_dev, dev_labels)))\n",
    "    #print(logreg.get_params())\n",
    "    \n",
    "    t0 = time()\n",
    "    logreg2 = logreg.fit(tf2_train, train_labels)\n",
    "    print(\"\\nLogreg fitting with preprocessing done in %0.3fs.\" % (time() - t0))\n",
    "    print(\"Score for dev data with preprocessing: %f\" % (logreg2.score(tf2_dev, dev_labels)))\n",
    "    #print(logreg2.get_params())\n",
    "\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "a. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "The vocabulary was reduced by 14.68% by preprocessing and cleaning up. This resulted in a 0.016272 increase in score using the dev_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "I believe I've maxed the options in Count_Vectorizer. The better_preprocessor filters out punctuation and underscores, keeps only \"words\" of 5+ length, and converts everything to lowercase. The code outputs the vocabulary size and scores for the dev data for models with and without preprocessing.\n",
    "\n",
    "I think I would want to do a check with a dictionary next to filter out garbage character strings.\n",
    "\n",
    "Logreg fitting done in 122.365s.\n",
    "Score for dev data: 0.707101\n",
    "\n",
    "Logreg fitting with preprocessing done in 34.522s.\n",
    "Score for dev data with preprocessing: 0.723373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizing done in 1.229s.\n",
      "\n",
      "Done in 5.431s.\n",
      "Number of nonzero weights for l1 logreg: 1,727\n",
      "\n",
      "Done in 14.893s.\n",
      "Number of nonzero weights for l2 logreg: 107,516\n",
      "\n",
      "         C  Vocab Size    L2 Score\n",
      "      0.01          16       0.475\n",
      "    100.01       7,469       0.680\n",
      "    200.01      11,769       0.689\n",
      "    300.01      14,718       0.697\n",
      "    400.01      17,418       0.691\n",
      "    500.01      19,068       0.698\n",
      "    600.01      20,263       0.689\n",
      "    700.01      21,431       0.698\n",
      "    800.01      22,335       0.685\n",
      "    900.01      22,950       0.694\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGHCAYAAACnPchFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118679e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6():\n",
    "# References:\n",
    "\n",
    "    # keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # get the training data vectorized\n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    tf_dev = tf_vectorizer.transform(dev_data)\n",
    "    print(\"\\nVectorizing done in %0.3fs.\" % (time() - t0))       \n",
    "    #print \"Training data shape: \", tf_train.shape\n",
    "    #print \"Training label shape: \", train_labels.shape\n",
    "    #print \"Training data type: \", type(tf_train)\n",
    "    #print \"Vectorized dev data shape: \", tf_dev.shape\n",
    "    #print \"Dev label shape: \", dev_labels.shape  \n",
    "    #print \"Dev data type: \", type(tf_dev)\n",
    "\n",
    "    # train a logistic regression model using a l1 penalty\n",
    "    logregl1CV = LogisticRegressionCV(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        tol=0.01,\n",
    "        Cs=np.linspace(0.01, 1.0, 10)\n",
    "    )   \n",
    "    t0 = time()\n",
    "    logregl1CV = logregl1CV.fit(tf_train, train_labels)\n",
    "    print(\"\\nDone in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    # output the number of learned weights that are not equal to zero\n",
    "    print(\"Number of nonzero weights for l1 logreg: %s\" % ('{:,}'.format(np.count_nonzero(logregl1CV.coef_))))\n",
    "\n",
    "    # train a logistic regression model using a l2 penalty\n",
    "    logregl2CV = LogisticRegressionCV(\n",
    "        penalty='l2',\n",
    "        solver='liblinear',\n",
    "        tol=0.01,\n",
    "        Cs=np.linspace(0.01, 1.0, 10)\n",
    "    )   \n",
    "    t0 = time()\n",
    "    logregl2CV = logregl2CV.fit(tf_train, train_labels)\n",
    "    print(\"\\nDone in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    # output the number of learned weights that are not equal to zero\n",
    "    print(\"Number of nonzero weights for l2 logreg: %s\" % ('{:,}'.format(np.count_nonzero(logregl2CV.coef_)))) \n",
    "    \n",
    "    l2_score = []\n",
    "    vocab_size = []\n",
    "    print('\\n%10s  %10s  %10s' % ('C', 'Vocab Size', 'L2 Score'))\n",
    "          \n",
    "    #for c in np.linspace(0.01, 100.0, 10):\n",
    "    for c in np.arange(0.01, 1000.0, 100):\n",
    "        \n",
    "        # train a logistic regression model using a l1 penalty  \n",
    "        logregl1 = LogisticRegression(\n",
    "            penalty='l1',\n",
    "            solver='liblinear',\n",
    "            tol=0.01,\n",
    "            C=c\n",
    "        )   \n",
    "        t0 = time()\n",
    "        logregl1 = logregl1.fit(tf_train, train_labels)\n",
    "        #print(\"\\nDone in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "        # output the number of learned weights that are nonzero\n",
    "        #print(\"Number of nonzero weights for l1 logreg with c of %f: %s\" % (c, '{:,}'.format(np.count_nonzero(logregl1.coef_))))\n",
    "        #print(\"Score for dev data: %f\" % (logregl1.score(tf_dev, dev_labels)))\n",
    "        \n",
    "        # reduce the size of vocabulary keeping only features that have at least one non-zero weight in l1 coef_\n",
    "        \n",
    "        # get indicies for nonzero values and append to unique list\n",
    "        trim_ind = set()\n",
    "        for row in logregl1.coef_:\n",
    "            inds = list(np.flatnonzero(row))\n",
    "            trim_ind = trim_ind.union(inds)      \n",
    "        #print len(trim_ind)\n",
    "        \n",
    "        trimmed_vocab = [tf_vectorizer.get_feature_names()[i] for i in trim_ind]\n",
    "        tf_vectorizer2 = CountVectorizer(vocabulary=trimmed_vocab)\n",
    "        t0 = time()\n",
    "        tf_train2 = tf_vectorizer2.fit_transform(train_data)\n",
    "        tf_dev2 = tf_vectorizer2.transform(dev_data)\n",
    "        #print(\"\\nVectorizing trimmed vocab done in %0.3fs.\" % (time() - t0))\n",
    "        #print tf_train2.shape\n",
    "        \n",
    "        # capture the vocab size\n",
    "        vocab_size.append(int(tf_dev2.shape[1]))\n",
    "        \n",
    "        # retrain a model using l2\n",
    "        \n",
    "        logregl2 = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            solver='liblinear',\n",
    "            tol=0.01,\n",
    "            C=c\n",
    "        )\n",
    "        t0 = time()\n",
    "        logregl2 = logregl2.fit(tf_train2, train_labels)\n",
    "        #print(\"\\nDone in %0.3fs.\" % (time() - t0))\n",
    "        # output the number of learned weights that are not equal to zero\n",
    "        #print(\"Number of nonzero weights for l2 logreg with c of %f: %s\" % (c, '{:,}'.format(np.count_nonzero(logregl2.coef_))))\n",
    "        \n",
    "        score = logregl2.score(tf_dev2, dev_labels)\n",
    "        #print(\"Score for dev data: %f\" % (score))   \n",
    "        \n",
    "        # capture the score\n",
    "        l2_score.append(float(score))\n",
    "        \n",
    "        # print the C, vocab size, l2_score\n",
    "\n",
    "        print('%10s  %10s  %10s' % (c, '{:,}'.format(int(tf_dev2.shape[1])),\n",
    "                                                    '{:.3f}'.format(score)))\n",
    "    \n",
    "    # make a plot showing accuracy (score) of retrained-model vs vocab size when pruning unused features by adjusting C param\n",
    "    #print len(l2_score)\n",
    "    #print len(vocab_size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(vocab_size, l2_score, 'ko')\n",
    "    plt.xlabel('vocab size')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('Score vs Vocab Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "a. How do the non-zero weights with \"l1\" compare to the number of non-zero weights you get with \"l2\"?\n",
    "\n",
    "When running logreg liblinear for l1 and l2, they have drastically difference numbers of nonzero weights in the coef matrix. l1 results in a much lower number of nonzero, depending on C value, as opposed to l2 which results in commonly driving all weights to nonzero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "This code takes a significant time to execute - keep this in mind when running.\n",
    "\n",
    "It took me far too long to understand what I was supposed to do in reducing the vocabulary and how to do it. The code for P6 is verbose which helps me see all steps in detail. \n",
    "\n",
    "The score improves as the vocab size increases. The vocab is driven by the l1 model, which presumably presents which features are of most use for each C value. By training an l2 model on the trimmed vocab, presumably the score should be relevant.\n",
    "\n",
    "I kept with a non-preprocessed input throughout the assignment unless stated otherwise. If I were doing the modeling for a project or assignment, I'd need to preprocess the data like what was doen in P5.\n",
    "\n",
    "For reference, here is the data output for a narrow range and default tol:\n",
    "\n",
    "         C  Vocab Size    L2 Score\n",
    "      0.01          16       0.475\n",
    "      0.12         240       0.683\n",
    "      0.23         398       0.680\n",
    "      0.34         553       0.676\n",
    "      0.45         655       0.685\n",
    "      0.56         769       0.679\n",
    "      0.67         835       0.667\n",
    "      0.78         872       0.669\n",
    "      0.89         917       0.673\n",
    "       1.0         972       0.680\n",
    "       \n",
    "         C  Vocab Size    L2 Score\n",
    "      0.01          16       0.475\n",
    "    100.01       2,928       0.688\n",
    "    200.01       3,079       0.686\n",
    "    300.01       3,464       0.682\n",
    "    400.01       3,439       0.675\n",
    "    500.01       3,447       0.683\n",
    "    600.01       3,845       0.689\n",
    "    700.01       3,913       0.691\n",
    "    800.01       3,993       0.678\n",
    "    900.01       4,389       0.688\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Pipeline for LogReg with Tfid\n",
      "('Pipeline:', ['tfid', 'logreg'])\n",
      "\n",
      "\n",
      "Logreg fitting done in 1.402s\n",
      "\n",
      "\n",
      "Classification report for TfidVectorizer and LogisticRegression Pipeline(steps=[('tfid', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.69      0.62      0.65       165\n",
      "     comp.graphics       0.80      0.91      0.85       185\n",
      "         sci.space       0.83      0.83      0.83       199\n",
      "talk.religion.misc       0.68      0.64      0.66       127\n",
      "\n",
      "       avg / total       0.76      0.76      0.76       676\n",
      "\n",
      "\n",
      "LogisticRegression f1_score: 0.763\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Worst R doc (929.358) was predicted comp.graphics but should have been talk.religion.misc:\n",
      "\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "-----------------------------\n",
      "\n",
      "Second worst R doc (325.004) was predicted comp.graphics but should have been talk.religion.misc:\n",
      "\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "-----------------------------\n",
      "\n",
      "Third worst R doc (287.179) was predicted talk.religion.misc but should have been alt.atheism:\n",
      "\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "def P7():\n",
    "# References:\n",
    "    \n",
    "    # use the TfidVectorizer\n",
    "\n",
    "    # set up the pipeline\n",
    "    logreg_pipe = Pipeline([\n",
    "        ('tfid', TfidfVectorizer()),\n",
    "        ('logreg', LogisticRegression(penalty='l2', C=100)),\n",
    "    ])\n",
    "\n",
    "    print(\"-----------Pipeline for LogReg with Tfid\")\n",
    "    print(\"Pipeline:\", [name for name, _ in logreg_pipe.steps])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # train a logistic regression model with C=100\n",
    "    \n",
    "    t0 = time()\n",
    "    logreg_pipe.fit(train_data, train_labels)\n",
    "    print(\"Logreg fitting done in %0.3fs\\n\" % (time() - t0))\n",
    "        \n",
    "    # make predictions on the dev data\n",
    "    \n",
    "    predicted = logreg_pipe.predict(dev_data)\n",
    "    expected = dev_labels\n",
    "    report = classification_report(expected, predicted, target_names=newsgroups_test.target_names)\n",
    "    print(\"\\nClassification report for TfidVectorizer and LogisticRegression %s:\\n%s\\n\" % (logreg_pipe, report))\n",
    "\n",
    "    #print(metrics.f1_score(expected, predicted, average=None))\n",
    "    #print(metrics.f1_score(expected, predicted, average='macro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='micro'))\n",
    "    #print(metrics.f1_score(expected, predicted, average='weighted'))\n",
    "    print(\"LogisticRegression f1_score: %0.3f\\n\" % (logreg_pipe.score(dev_data, dev_labels)))\n",
    "    \n",
    "    # show the top 3 documents where the ratio R is largest\n",
    "    # R = max predicted probability divided by predicted probability of the correct label\n",
    "    \n",
    "    proba_results = logreg_pipe.predict_proba(dev_data)\n",
    "    results = []\n",
    "    r = 0\n",
    "    for probs in proba_results:\n",
    "        results.append([r, np.argmax(probs), dev_labels[r], max(probs)/probs[dev_labels[r]], max(probs), probs[dev_labels[r]]])\n",
    "        r = r + 1\n",
    "\n",
    "    results.sort(key=lambda x: x[3])\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"\\nWorst R doc (%.3f) was predicted %s but should have been %s:\\n\" % (results[-1][3],\n",
    "                                                                           newsgroups_test.target_names[results[-1][1]], \n",
    "                                                                           newsgroups_test.target_names[results[-1][2]]))\n",
    "    print dev_data[results[-1][0]]\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"\\nSecond worst R doc (%.3f) was predicted %s but should have been %s:\\n\" % (results[-2][3],\n",
    "                                                                           newsgroups_test.target_names[results[-2][1]], \n",
    "                                                                           newsgroups_test.target_names[results[-2][2]]))\n",
    "    print dev_data[results[-2][0]]\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"\\nThird worst R doc (%.3f) was predicted %s but should have been %s:\\n\" % (results[-3][3],\n",
    "                                                                           newsgroups_test.target_names[results[-3][1]], \n",
    "                                                                           newsgroups_test.target_names[results[-3][2]]))\n",
    "    print dev_data[results[-3][0]]\n",
    "    print(\"-----------------------------\")   \n",
    "    \n",
    "P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "a. How is the TfidfVectorizer different from the CountVectorizer?\n",
    "\n",
    "TfidfVectorizer emphasizes features that appear in frequency in a document rather than frequently across many documents. CountVectorizer performs a count of words across documents without emphasis. This helps explain why Tfid performed better than CountVectorizer for Logistic Regression. Tfid drove the Logistic Regression score closer to Naive Bayes. Tfid helps emphasize word frequency rather than generically across all documents equally.\n",
    "\n",
    "b. What kind of mistakes is the model making?\n",
    "\n",
    "The worse and second worst mistakes, according to the R value, was predicted to be graphics but should have been religion. There appears to have been a number of technical terms in the document that swayed the prediction to be technical over the religious topic. The third worse document was close in topics.\n",
    "\n",
    "c. Suggest a way to address on particular issue that you see.\n",
    "\n",
    "I would like to see how preprocessing would influence these results. This may help refine the score, but isn't necessarily going to address the context versus content issue seen in the worst performing R documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "Logistic Regression with the vanilla TfidfVectorizer actually outperformed Naive Bayes. \n",
    "\n",
    "It isn't immediately obvious to me as to how to address the issue of context versus content. If I am writing a letter about religion, but including instructions on how to download religious documents which would weight more heavily with a technical topic prediction, how do I instruct a machine to decipher context rather than content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing done in 1.712s.\n",
      "\n",
      "The 2,034 document collection has a 22,933 token vocabulary.\n",
      "\n",
      "\n",
      "Logreg fitting done in 21.309s.\n",
      "Score for dev data using Logreg with Tfid: 0.739645\n"
     ]
    }
   ],
   "source": [
    "def better_preprocessor(s):\n",
    "    # strip punctuation, underscores, and short words, and lowercase the resulting words\n",
    "    return re.sub(ur'\\b\\w{1,4}\\b','',re.sub(ur\"[^\\w\\d\\s]+|\\d+|_+\",'',s)).lower()\n",
    "\n",
    "def P8():\n",
    "    \n",
    "    # vectorize the raw data\n",
    "    tf_vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True,\n",
    "        preprocessor=better_preprocessor,\n",
    "        stop_words='english',\n",
    "        max_df=0.8,\n",
    "    )\n",
    "    t0 = time()\n",
    "    tf_train = tf_vectorizer.fit_transform(train_data)\n",
    "    tf_dev = tf_vectorizer.transform(dev_data)\n",
    "    print(\"Vectorizing done in %0.3fs.\\n\" % (time() - t0))\n",
    "    #print(tf2_train.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(tf2_dev.shape)\n",
    "    #print(dev_labels.shape)\n",
    "    \n",
    "    # what is the size of the vocabulary?\n",
    "    print(\"The %s document collection has a %s token vocabulary.\\n\" % ('{:,}'.format(tf_train.shape[0]), \n",
    "                                                                     '{:,}'.format(tf_train.shape[1])))\n",
    "    \n",
    "    # calculate the scores using dev_data for comparison\n",
    "    logreg = LogisticRegressionCV(\n",
    "        Cs=np.linspace(0.01, 1.0, 20)\n",
    "    )\n",
    "    t0 = time()\n",
    "    logreg = logreg.fit(tf_train, train_labels)\n",
    "    print(\"\\nLogreg fitting done in %0.3fs.\" % (time() - t0))\n",
    "    print(\"Score for dev data using Logreg with Tfid: %f\" % (logreg.score(tf_dev, dev_labels)))\n",
    "\n",
    "P8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "Preprocessing was not helpful at all here. It actually lowered the score. \n",
    "\n",
    "This isn't a particularly fabulous way to address the context vs content issue. I just wanted to see if normalizing input would help at all.\n",
    "\n",
    "I would love to discuss this homework in class."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
